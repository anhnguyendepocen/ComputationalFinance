{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 300%\"> Computational Finance </p>\n",
    "<img src=\"img/ABSlogo.svg\" alt=\"LOGO\" style=\"display:block; margin-left: auto; margin-right: auto; width: 50%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#silence some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plotting Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Risk Measures\n",
    "## Introduction\n",
    "* The Basel Accords mandate that financial institutions report the risk associated with their positions, so that regulators may check the adequacy of the economic capital as a buffer against market risk.\n",
    "* Reporting is the form of a *risk measure*, which condenses the risk of a position into a single number.\n",
    "* Currently, the mandated measure is *Value at Risk*, but there are debates of replacing it with an alternative (*Expected Shortfall*).\n",
    "* Banks are allowed to use their own, internal models for the computation of VaR, but the adequacy of these models should be *backtested*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value at Risk\n",
    "* Consider a portfolio with value $V_{PF,t}$ and daily returns $%\n",
    "R_{PF,t}$.\n",
    "* Define the one-day Loss on the portfolio as\n",
    "\\begin{equation*}\n",
    "\\$Loss_{t+1}=V_{PF,t}-V_{PF,t+1}.\n",
    "\\end{equation*}\n",
    "* I will distinguish between the dollar Value at Risk (an amount) and the return Value at Risk (a percentage). When unqualified, I mean the latter.\n",
    "* The one-day $100p\\%$ dollar Value at Risk\n",
    "$\\$VaR_{t+1}^{p}$ is the loss on the portfolio that we we are $100\\left(1-p\\right) \\%$ confident will not be exceeded. The Basel committee requires $p=0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The *return Value at risk* $VaR_{t+1}^{p}$ expresses $\\$VaR_{t+1}^{p}$ as a percentage of the portfolio value:\n",
    "\\begin{equation*}\n",
    "VaR_{t+1}^{p}=\\frac{\\$VaR_{t+1}^{p}}{V_{PF,t}}.\n",
    "\\end{equation*}\n",
    "* Hence\n",
    "\\begin{equation*}\n",
    "\\Pr (R_{PF,t+1}<-VaR_{t+1}^{p})=p,\n",
    "\\end{equation*}\n",
    "because\n",
    "\\begin{equation*}\n",
    "R_{PF,t+1}=-\\frac{\\$Loss_{t+1}}{V_{PF,t}}.\n",
    "\\end{equation*}\n",
    "\n",
    "Thus $VaR_{t+1}^{p}$ is minus the $100p$th *percentile*\n",
    "of the return distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import scipy.stats as stats #The book likes to import it as `scs`\n",
    "a, b, c = -5, 5, stats.norm.ppf(0.05)\n",
    "x = np.linspace(a, b, 100)\n",
    "y = stats.norm.pdf(x)\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(x, y, 'b', linewidth=2)\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlim(xmin=a, xmax=b)\n",
    "Ix = np.linspace(a, c)\n",
    "Iy = stats.norm.pdf(Ix)\n",
    "verts = [(a, 0)] + list(zip(Ix, Iy)) + [(c, 0)]\n",
    "poly = Polygon(verts, facecolor='0.7', edgecolor='0.5')\n",
    "ax.add_patch(poly)\n",
    "ax.annotate('$p\\%$', xy=(-2, 0.025), xytext=(-3, 0.1),\n",
    "            arrowprops=dict(width=.5),\n",
    "            )\n",
    "plt.xlabel('$R_{PF,t+1}$')\n",
    "plt.ylabel('$f(R_{PF,t+1})$')\n",
    "\n",
    "ax.set_xticks([c, 0])\n",
    "ax.set_xticklabels(['$-VaR_{t+1}^p$', '0'])\n",
    "ax.set_yticks([])\n",
    "plt.savefig('img/var.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/var.svg\" alt=\"VaR\" style=\"display:block; margin-left: auto; margin-right: auto; width: 80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Asset Returns: Stylized Facts\n",
    "* Stylized facts about asset returns include\n",
    "  * Lack of autocorrelation\n",
    "  * Leverage effects\n",
    "  * Heavy tails of returns distribution\n",
    "  * Volatility clustering\n",
    "  \n",
    "* These need to be taken into account when creating VaR forecasts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VaR Methods: Unconditional\n",
    "### Non-parametric: Historical Simulation\n",
    "\n",
    "* Historical simulation assumes that the distribution of tomorrow's portfolio\n",
    "returns is well approximated by the empirical distribution (histogram) of\n",
    "the past $m$ observations $\\left\\{\n",
    "R_{PF,t},R_{PF,t-1},\\ldots,R_{PF,t+1-m}\\right\\} $.\n",
    "\n",
    "* This is as if we draw, with replacement, from the last $m$ returns and use\n",
    "this to simulate the next day's return distribution.\n",
    "\n",
    "* The estimator of VaR is given by minus the $100p$th *percentile* (or the $p$th *quantile*) of the sequence of past portfolio returns, i.e., $\\widehat{VaR}_{t+1}^{p}=-R_{t+1}^{p}$, where $R_{t+1}^{p}$ is the number such that $100p\\%$ of the observations are smaller than $R_{t+1}^{p}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In Python, we can use NumPy's `quantile` method, or the `percentile` function (or `nanpercentile` which ignores `NaN`s). Hilpisch uses `scoreatpercentile`, but that is is deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024078428377579123"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "from datetime import timedelta\n",
    "m=1000 #~4 trading years\n",
    "p=web.DataReader(\"^GSPC\", 'yahoo', end=pd.datetime.today(), start=pd.datetime.today()-timedelta(days=1000))['Adj Close']\n",
    "r=np.log(p)-np.log(p).shift(1)\n",
    "r.name='Return'\n",
    "r=r[1:] #remove first observation (NaN)\n",
    "VaR_hist=-r.quantile(.01) #Alternatively, VaR=np.percentile(r,1)\n",
    "VaR_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ax=r.hist(bins=30) #histogram with 30 bins\n",
    "ax.set_xticks([-VaR_hist])\n",
    "ax.set_xticklabels(['$-VaR_{t+1}^{0.01}$ = -%4.3f' %VaR_hist]) #4.3f means 4 digits, of which 3 decimals\n",
    "plt.title('Historical VaR')\n",
    "plt.savefig('img/var_hist.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/var_hist.svg\" alt=\"VaR_hist\" style=\"display:block; margin-left: auto; margin-right: auto; width: 80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Problem: Last year(s) of data not necessarily representative for the next few days (e.g. because of volatility clustering).\n",
    "* Exacerbated by the fact that a large $m$ is required to compute 1% VaR with any degree of precision (only 1% of the data are really used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parametric: Normal and $t$ Distributions\n",
    "* Another simple approach is to\n",
    "assume $R_{t+1}=R_{PF,t+1}\\sim N(\\mu ,\\sigma ^{2})$, and to estimate $\\mu $\n",
    "and $\\sigma ^{2}$ using historical data (for daily data, $\\mu \\approx 0$).\n",
    "\n",
    "* The VaR is then determined from\n",
    "\\begin{eqnarray*}\n",
    "\\Pr \\left( R_{t+1}<-VaR_{t+1}^{p}\\right) &=&\\Pr \\left( \\frac{R_{t+1}-\\mu }{%\n",
    "\\sigma }<\\frac{-VaR_{t+1}^{p}-\\mu }{\\sigma }\\right) \\\\\n",
    "&=&\\Pr \\left( z_{t+1}<\\frac{-VaR_{t+1}^{p}-\\mu }{\\sigma }\\right) \\\\\n",
    "&=&\\Phi \\left( \\frac{-VaR_{t+1}^{p}-\\mu }{\\sigma }\\right) =p,\n",
    "\\end{eqnarray*}\n",
    "where $\\Phi (z)$ is the cumulative standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Thus,\n",
    "\\begin{equation*}\n",
    "VaR_{t+1}^{p}=-\\mu -\\sigma \\Phi^{-1}(p)\n",
    "\\end{equation*}\n",
    "where $\\Phi ^{-1}(p)$ is the inverse distribution function of the standard normal, a.k.a. the percentage point function (ppf).\n",
    "* In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018156277996617072"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu, sig=stats.norm.fit(r) #fit a normal distribution to r\n",
    "VaR_norm=-mu-sig*stats.norm.ppf(0.01)\n",
    "VaR_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sbs\n",
    "ax=sbs.distplot(r, kde=False, fit=stats.norm) #histogram overlaid with fitted normal density\n",
    "ax.set_xticks([-VaR_norm])\n",
    "ax.set_xticklabels(['$-VaR_{t+1}^{0.01}$ = -%4.3f' %VaR_norm])\n",
    "ax.text(0.02,60,'$\\mu=%7.6f$\\n$\\sigma=%7.6f$' %(mu, sig)) #\\n is newline\n",
    "plt.title('Normal VaR')\n",
    "plt.savefig('img/var_norm.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/var_norm.svg\" alt=\"VaR_norm\" style=\"display:block; margin-left: auto; margin-right: auto; width: 80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Problems:\n",
    "  * Variance of the past year(s) of data not necessarily representative for the future.\n",
    "  * Returns typically have heavier tails than the normal.\n",
    "* The solution to the second point is to use another distribution. The Student's $t$ distribution is a popular choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The Student's $t$ distribution with $\\nu$ degrees of freedom, $t_\\nu$, is well known from linear regression\n",
    "as the distribution of $t$-statistics, where $\\nu=T-k$.\n",
    "\n",
    "* Can be generalized to allow $\\nu\\in\\mathbb{R}_+$.\n",
    "\n",
    "* Small values of $\\nu$ correspond to fat tails. As $\\nu\\rightarrow \\infty $, we approach the $N(0,1)$ distribution.\n",
    "\n",
    "* It only has moments up to but not including $\\nu$:\n",
    "  * The mean is finite only for $\\nu>1$.\n",
    "  * The variance is finite for $\\nu>2$ and given by $\\nu/(\\nu-2)$.\n",
    "  * The excess kurtosis is finite for $\\nu>4$ and given by $6/(\\nu-4)$.\n",
    "\n",
    "* The distributions are symmetric around $0$, hence mean and skewness are $0$ if they exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,510)\n",
    "df=[1, 2, 3, 10]\n",
    "for nu in df:\n",
    "    plt.plot(x, stats.t.pdf(x, nu))\n",
    "legend=['$\\\\nu=%1.0f$' % nu for nu in df] #need double escaping: \\\\nu, not \\nu, because \\n is newline\n",
    "plt.plot(x, stats.norm.pdf(x))\n",
    "legend.append('Normal')\n",
    "plt.legend(legend)\n",
    "plt.savefig('img/tdists.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/tdists.svg\" alt=\"t distributions\" style=\"display:block; margin-left: auto; margin-right: auto; width: 80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* For financial applications, we need to allow for a non-zero mean, and a variance different from $\\nu/(\\nu-2)$.\n",
    "* This is achieved by introducing a *location parameter* $\\mu$ and a *scale parameter* $\\sigma$. We'll write $f_\\nu(x;\\mu,\\sigma)$ for the resulting density, $F_\\nu(x;\\mu,\\sigma)$ for the distribution function, and $F^{-1}_\\nu(p;\\mu,\\sigma)$ for the percentage point function.\n",
    "* Note that if $x\\sim t_\\nu(\\mu,\\sigma)$, $\\nu>2$, then $\\mathbb{E}[x]=\\mu$ and $\\mathrm{var}[x]=\\sigma^2\\nu/(\\nu-2)$.\n",
    "* The VaR becomes\n",
    "\\begin{equation*}\n",
    "VaR_{t+1}^{p}=-\\mu -\\sigma F^{-1}_\\nu(p;\\mu,\\sigma).\n",
    "\\end{equation*}\n",
    "* In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025029143273564612"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, mu, sig=stats.t.fit(r) #fit a location-scale t distribution to r\n",
    "VaR_t=-mu-sig*stats.t.ppf(0.01, df)\n",
    "VaR_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ax=sbs.distplot(r, kde=False, fit=stats.t) #histogram overlaid with fitted t density\n",
    "ax.set_xticks([-VaR_t])\n",
    "ax.set_xticklabels(['$-VaR_{t+1}^{0.01}$ = -%4.3f' %VaR_t])\n",
    "ax.text(0.02,60,'$\\mu=%7.6f$\\n$\\sigma=%7.6f$\\n$\\\\nu=%7.6f$' %(mu, sig, df))\n",
    "plt.title(\"Student's $t$ VaR\")\n",
    "plt.savefig('img/var_t.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/var_t.svg\" alt=\"VaR_t\" style=\"display:block; margin-left: auto; margin-right: auto; width: 80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* There are several ways to assess whether a distributional assumption is adequate.\n",
    "* One is to use a *goodness of fit test*. Many such tests exist.\n",
    "* Hilpisch discusses the D'Agostino-Pearson test, available as `stats.normaltest`. Here we use the Jarque-Bera test.\n",
    "* The test statistic is\n",
    "$\n",
    "JB=N\\left(S^2/6+(K-3)/24\\right),\n",
    "$\n",
    "where $S$ and $K$ are respectively the sample skewness and kurtosis.\n",
    "* Intuitively, it tests that the skewness and excess kurtosis are zero.\n",
    "* It is distributed as $\\chi^2_2$ under the null of normality. The 5% critical value is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9914645471079799"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.chi2.ppf(0.95, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(348.21197524800846, 0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.jarque_bera(r) #returns (JB, p-val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Another option is to use a QQ-plot (quantile-quantile plot).\n",
    "* It plots the empirical quantiles against the quantiles of a hypothesized distribution, e.g. $\\Phi^{1}(p)$ for the normal.\n",
    "* If the distributional assumption is correct, then the plot should trace out the 45 degree line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#This is the manual way to do it.\n",
    "%matplotlib inline\n",
    "x=np.linspace(.01,.99)\n",
    "emp=r.quantile(x)\n",
    "mu, sig=stats.norm.fit(r)\n",
    "theo=stats.norm.ppf(x, mu, sig)\n",
    "ax=plt.plot(theo, emp.values, 'o')\n",
    "#plt.xlim(xmin=-3, xmax=3)\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Sample Quantiles')\n",
    "plt.title('QQ Plot vs. Normal')\n",
    "plt.savefig('img/qq_norm.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/qq_norm.svg\" alt=\"QQ Plot\" style=\"display:block; margin-left: auto; margin-right: auto; width: 80%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#this is a bit simpler\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "sm.qqplot(r, dist=stats.t, fit=True)\n",
    "plt.title(\"QQ Plot vs. Student's $t$\")\n",
    "plt.savefig('img/qq_t.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/qq_t.svg\" alt=\"QQ Plot\" style=\"display:block; margin-left: auto; margin-right: auto; width: 80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VaR Methods: Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myewm=r.ewm(ignore_na=False,min_periods=0,adjust=True,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig=myewm.var(bias=False)\n",
    "sig.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rolling estimation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
